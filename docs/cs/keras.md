# ğ™†ğ™šğ™§ğ™–ğ™¨ åŸºç¡€

::: tip å‚è€ƒæ–‡æ¡£
[Keras å®˜ç½‘æ–‡æ¡£](https://keras.io)
:::

## å®‰è£…

```bash
# å®‰è£… Tensorflow
pip install tensorflow --user -i https://pypi.tuna.tsinghua.edu.cn/simple
# å®‰è£… Keras
pip install keras --user -i https://pypi.tuna.tsinghua.edu.cn/simple
```

## çº¿æ€§å›å½’

### ç¤ºä¾‹ç¨‹åº
```python
# -*- coding:utf-8 -*-
import numpy as np
import matplotlib.pyplot as plt
import keras
from keras.models import Sequential  # Keras ä¸­çš„é¡ºåºæ¨¡å‹
from keras.layers import Dense  # Keras ä¸­çš„å…¨è¿æ¥å±‚

# æ„é€ è®­ç»ƒæ ·æœ¬
x_train = np.random.rand(100)  # ä¸€ç»´æ•°æ®ï¼Œæ ·æœ¬é‡ 100ï¼Œæœä»å‡ä¸€åˆ†å¸ƒ
noise = np.random.normal(0, 0.01, x_train.shape)  # å™ªå£°æ•°æ®ï¼Œæœä»é«˜æ–¯åˆ†å¸ƒï¼ˆæ­£æ€åˆ†å¸ƒï¼‰
y_train = x_train*0.1+0.2 + noise

# ç¼–è¯‘æ¨¡å‹
model = Sequential()  # é¡ºåºæ¨¡å‹
model.add(Dense(units=1, input_dim=1))  # å…¨è¿æ¥å±‚ï¼Œè¾“å‡ºæ•°æ® 1 ç»´ï¼Œ è¾“å…¥æ•°æ® 1 ç»´
model.compile(optimizer="sgd", loss="mse")  # ä¼˜åŒ–ç®—æ³•ï¼šéšæœºæ¢¯åº¦ä¸‹é™ï¼ŒæŸå¤±å‡½æ•°ï¼šå‡æ–¹å·®

# è®­ç»ƒæ ·æœ¬
for step in range(5000):
    cost = model.train_on_batch(x_train, y_train)
    if step % 500 == 0:
        print("cost: ", cost)

# æŸ¥çœ‹è®­ç»ƒå¾—åˆ°çš„æƒé‡å’Œåç½®
w, b = model.layers[0].get_weights()
print("W= ", w, "b= ", b)

# å±•ç¤ºæ‹Ÿåˆç»“æœ
y_pred = model.predict(x_train)
plt.scatter(x_train, y_train)
plt.plot(x_train, y_pred, "r-", lw=3)
plt.show()
```

### ç»“æœå±•ç¤º

![linear_regression](../.vuepress/public/images/cs/keras/linear_regression.png)

## éçº¿æ€§å›å½’

### ç¤ºä¾‹ç¨‹åº

```python
# -*- coding:utf-8 -*-
import numpy as np
import matplotlib.pyplot as plt
import keras
from keras.models import Sequential  # Keras ä¸­çš„é¡ºåºæ¨¡å‹
from keras.layers import Dense, Activation  # Keras ä¸­çš„å…¨è¿æ¥å±‚å’Œæ¿€æ´»å‡½æ•°
from keras.optimizers import SGD  # Keras ä¸­çš„ SGD ä¼˜åŒ–å™¨

# æ„é€ è®­ç»ƒæ ·æœ¬
x_train = np.linspace(-0.5, 0.5, 200)  # ä¸€ç»´æ•°æ®ï¼Œæ ·æœ¬é‡ 200ï¼Œç­‰å·®çº¿æ€§åˆ†å¸ƒ
noise = np.random.normal(0, 0.02, x_train.shape)  # å™ªå£°æ•°æ®ï¼Œæœä»é«˜æ–¯åˆ†å¸ƒï¼ˆæ­£æ€åˆ†å¸ƒï¼‰
y_train = np.square(x_train) + noise

# ç¼–è¯‘æ¨¡å‹
model = Sequential()  # é¡ºåºæ¨¡å‹
model.add(Dense(units=10, input_dim=1))  # å…¨è¿æ¥å±‚ï¼Œè¾“å‡ºæ•°æ® 10 ç»´ï¼Œ è¾“å…¥æ•°æ® 1 ç»´
model.add(Activation("tanh"))  # tanh æ¿€æ´»å‡½æ•°
model.add(Dense(units=1))  # å…¨è¿æ¥å±‚ï¼Œè¾“å‡ºæ•°æ® 1 ç»´ï¼Œ è¾“å…¥æ•°æ® 10 ç»´
model.add(Activation("tanh"))  # tanh æ¿€æ´»å‡½æ•°
sgd = SGD(lr=0.2)  # è®¾ç½®éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•çš„å­¦ä¹ ç‡
model.compile(optimizer=sgd, loss="mse")  # ä¼˜åŒ–ç®—æ³•ï¼šéšæœºæ¢¯åº¦ä¸‹é™ï¼ŒæŸå¤±å‡½æ•°ï¼šå‡æ–¹å·®

# è®­ç»ƒæ ·æœ¬
for step in range(5000):
    cost = model.train_on_batch(x_train, y_train)
    if step % 500 == 0:
        print("cost: ", cost)

# å±•ç¤ºæ‹Ÿåˆç»“æœ
y_pred = model.predict(x_train)
plt.scatter(x_train, y_train)
plt.plot(x_train, y_pred, "r-", lw=3)
plt.show()
```

### ç»“æœå±•ç¤º

![nonlinear_regression](../.vuepress/public/images/cs/keras/nonlinear_regression.png)

## MNIST æ•°æ®åˆ†ç±»

### ç¤ºä¾‹ç¨‹åº

```python
# -*- coding:utf-8 -*-
import numpy as np
import keras
from keras.datasets import mnist
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.optimizers import SGD

# è½½å…¥æ•°æ®é›†ï¼ˆç¬¬ä¸€æ¬¡è¿è¡Œä¼šè”ç½‘ä¸‹è½½ï¼‰
(x_train, y_train), (x_test, y_test) = mnist.load_data()
print("x_train: ", x_train.shape, "y_train: ", y_train.shape)
print("x_test:  ", x_test.shape, "y_test:  ", y_test.shape)
x_train = x_train.reshape(x_train.shape[0], -1)/255.0  # å½’ä¸€åŒ–
x_test = x_test.reshape(x_test.shape[0], -1)/255.0  # æ ‡ç­¾å€¼è½¬ä¸º one-hot ç 
y_train = np_utils.to_categorical(y_train, num_classes=10)
y_test = np_utils.to_categorical(y_test, num_classes=10)

# ç¼–è¯‘æ¨¡å‹
model = Sequential()  # é¡ºåºæ¨¡å‹
# å…¨è¿æ¥å±‚(éšè—å±‚ï¼‰ï¼Œè¾“å‡ºæ•°æ® 200 ç»´ï¼Œè¾“å…¥æ•°æ®ç»´åº¦ç”±è®­ç»ƒæ•°æ®æœ¬èº«å†³å®š, ä½¿ç”¨ tanh æ¿€æ´»å‡½æ•°
model.add(Dense(units=200, input_dim=x_train.shape[1], activation="tanh"))
model.add(Dropout(0.4))  # è®© 40% çš„ç¥ç»å…ƒä¸å·¥ä½œï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆçš„ä¸€ç§æ‰‹æ®µï¼‰
# å…¨è¿æ¥å±‚ï¼ˆéšè—å±‚ï¼‰ï¼Œè¾“å‡ºæ•°æ® 100 ç»´ï¼Œè¾“å…¥æ•°æ®ç”±å‰ä¸€å±‚å†³å®š, ä½¿ç”¨ tanh æ¿€æ´»å‡½æ•°
model.add(Dense(units=100, activation="tanh"))
model.add(Dropout(0.4))  # è®© 40% çš„ç¥ç»å…ƒä¸å·¥ä½œï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆçš„ä¸€ç§æ‰‹æ®µï¼‰
# å…¨è¿æ¥å±‚ï¼ˆè¾“å‡ºå±‚ï¼‰ï¼Œè¾“å‡ºæ•°æ® 10 ç»´ï¼ˆå…± 10 ç§æ‰‹å†™æ•°å­—ï¼‰ï¼Œè¾“å…¥æ•°æ®ç”±å‰ä¸€å±‚å†³å®š, ä½¿ç”¨ softmax æ¿€æ´»å‡½æ•°
model.add(Dense(units=10, activation="softmax"))
sgd = SGD(lr=0.2)  # è®¾ç½®éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•çš„å­¦ä¹ ç‡
model.compile(optimizer=sgd, loss="categorical_crossentropy", metrics=[
              "accuracy"])  # ä¼˜åŒ–ç®—æ³•ï¼šéšæœºæ¢¯åº¦ä¸‹é™ï¼ŒæŸå¤±å‡½æ•°ï¼šäº¤å‰ç†µ

# è®­ç»ƒæ ·æœ¬
model.fit(x_train, y_train, batch_size=32, epochs=10)

# è¯„ä¼°æ¨¡å‹
loss, accuracy = model.evaluate(x_test, y_test)
print("loss= ", loss, "accuracy= ", accuracy)
```
